{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M3A1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8o-CXS27Nu7"
      },
      "source": [
        "#Importing packages and the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eesYnXh4e6xP"
      },
      "source": [
        "#Import potential important packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqFfur7_A5Cy"
      },
      "source": [
        "#Importing neural network libraries\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, GRU, LSTM, Bidirectional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn0T9lOceVJz"
      },
      "source": [
        "#Importing the data. I'm lazy so lets just call the data for df, because d and f are right next to each other on the keyboard. How fun.\n",
        "df_url = \"https://github.com/SDS-AAU/SDS-master/raw/e2c959494d53859c1844604bed09a28a21566d0f/M3/assignments/trump_vs_GPT2.gz\"\n",
        "df = pd.read_json(df_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRCupDC9JmlW"
      },
      "source": [
        "#Preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcNTcDn8evgc",
        "outputId": "5928a8e3-dc9b-4e11-cbba-4440ebcd25e2"
      },
      "source": [
        "#Lets look at the data itself, first by looking at the shape, which gives us 14736 observations (tweets) and 2 features (tweet and label)\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14736, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 478
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "B7aq086c9iRX",
        "outputId": "4fc22b92-4dd9-4046-a66f-34d4f031268a"
      },
      "source": [
        "#Top 5 results, really just shows random tweets and labels shows us that all of them really are Trump-tweets. \n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I was thrilled to be back in the Great city of...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The Unsolicited Mail In Ballot Scam is a major...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>As long as I am President, I will always stand...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Our Economy is doing great, and is ready to se...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>If I do not sound like a typical Washington po...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0     1\n",
              "0  I was thrilled to be back in the Great city of...  True\n",
              "1  The Unsolicited Mail In Ballot Scam is a major...  True\n",
              "2  As long as I am President, I will always stand...  True\n",
              "3  Our Economy is doing great, and is ready to se...  True\n",
              "4  If I do not sound like a typical Washington po...  True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 479
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwBQTLURB1r-"
      },
      "source": [
        "#Because I like the beautiness of it, rather than just having the columns called 0 and 1, lets rename the columns. \n",
        "df.columns = ['tweet','label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk452IEzL1bc",
        "outputId": "ede92d3c-83f3-43c5-e4af-38235ae64f13"
      },
      "source": [
        "#And to check that there are equally amounts of true and false tweets\n",
        "df.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True     7368\n",
              "False    7368\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 481
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZa1ntyIfuk2",
        "outputId": "38abb82c-ae84-4659-f2de-ff910d91102b"
      },
      "source": [
        "#As we want to be able to test our model on a fresh dataset, we split the data-sets into a training data-set and a test set.\n",
        "14736 * 0.8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11788.800000000001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 482
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq0GWUVSFqH7"
      },
      "source": [
        "#Since the dataset has all the true tweets first and the false at the end, we will sample the dataset, meaning shuffle the observations. \n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MF3GhkJhevB"
      },
      "source": [
        "#Now we subset the datasets. We use 11788, as that is where it splits on 80% of the dataset.\n",
        "train = df.iloc[:11788,:]\n",
        "test = df.iloc[11788:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idiemxj3aXNg"
      },
      "source": [
        "#We are going to do some mergers later, so we need to reset the index, to be able to easily merge later on. \n",
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fbRB_W0DyaF"
      },
      "source": [
        "#We want to find out what to use as the max_features, so lets use the amount of characters in each tweet to determine that.\n",
        "#First we create a loop, that iterates each tweet, and counts the digets, and adds it to a column in the dataset.\n",
        "chars = []\n",
        "[chars.append(len(str(i))) for i in train['tweet']]\n",
        "train['chars'] = chars"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "4ibSEy_wE4vE",
        "outputId": "44feb776-44d5-4c88-c70c-d1ae95e92a98"
      },
      "source": [
        "#Now we check for the extra column\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>chars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Our big Kentucky Rally on Monday night had a m...</td>\n",
              "      <td>True</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bolton said “I don’t know what they’re talking...</td>\n",
              "      <td>False</td>\n",
              "      <td>187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It would show great weakness if Israel allowed...</td>\n",
              "      <td>True</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>“Double Standard - Former FBI lawyer (Lisa Pag...</td>\n",
              "      <td>True</td>\n",
              "      <td>103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LAST thing the Make America Great Again Agenda...</td>\n",
              "      <td>True</td>\n",
              "      <td>136</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  label  chars\n",
              "0  Our big Kentucky Rally on Monday night had a m...   True     80\n",
              "1  Bolton said “I don’t know what they’re talking...  False    187\n",
              "2  It would show great weakness if Israel allowed...   True     80\n",
              "3  “Double Standard - Former FBI lawyer (Lisa Pag...   True    103\n",
              "4  LAST thing the Make America Great Again Agenda...   True    136"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 487
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtKV6-M9FDOc",
        "outputId": "0b48063a-b714-4f89-857f-329300c1680f"
      },
      "source": [
        "#I firstly checked the max of the length, because I thought twitter only allowed 140 characters. Then I learned it was actually 280 chars. The reason some tweets are above, is because & and other symbols are written as &amp.\n",
        "max(train['chars'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "296"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 488
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2Ry49z5KshC",
        "outputId": "661e1d37-3ae5-468a-b753-301eb740bb2a"
      },
      "source": [
        "#Just for fun aswell, lets look at the minimum value. Shortest one have 0 chars. Odd.\n",
        "min(train['chars'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 489
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veazWWP8LdTv",
        "outputId": "eb45ac8c-0120-4f1e-b088-4c50c3fc9f4c"
      },
      "source": [
        "#Now lets see how many of these there are. \n",
        "train_min_chars = train[train.chars == 0]\n",
        "train_min_chars.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False    328\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 490
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEcFi3JrP56t"
      },
      "source": [
        "train = train[train.chars > 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzrR66jXMSbF"
      },
      "source": [
        "Okay, so this to me is a breakpoint. At this moment I can't decide. There are a couple hundreds GTP-2 created tweets, all just empty. Removing them only change the network such a tiny amount that removing them is hardly justified, as we in general don't want sqewed data, but it just bothers me that they exist. I've chosen to remove them. Sue me.\n",
        "\n",
        "This all began, because I wanted to use the average amount of characters as the max-features, but looking at it now, I don't think that 300 as the max length of the words is that bad. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHraRrhlNfJl"
      },
      "source": [
        "max_words = max(train['chars'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsIJ4z2bDyvT"
      },
      "source": [
        "#Tokenizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6p9s5xgRpvk"
      },
      "source": [
        "Lets be real - this way of tokenizing is hijacked directly from the task. Thank you Daniel / Roman / Eskil."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nEFtFVw88KN"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train['tweet'])\n",
        "sequences = tokenizer.texts_to_sequences(train['tweet'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFlUvBayQjRh"
      },
      "source": [
        "#Now we need to pad the tweets, to make them all equal length\n",
        "X = sequences\n",
        "X = pad_sequences(sequences = sequences, maxlen = max_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY_Kpe7ZRI5M"
      },
      "source": [
        "#Set y as the label, as that is the \"result\" we're going for\n",
        "y = train['label'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqoug2bWR78w",
        "outputId": "e4bbe8cc-9e2e-4acf-fb92-6722cb818080"
      },
      "source": [
        "#For good practice, lets check that they are the same size. They're.\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11460, 296)\n",
            "(11460,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4Yb_y9XSOUr"
      },
      "source": [
        "#Splitting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR5HgrlFUPgx"
      },
      "source": [
        "#We want to split the data, so that we have something to validate onto. For this we can use the good old train_test_split, and 0.2 as our test_size.\n",
        "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 1337)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eHopS4sUlr6"
      },
      "source": [
        "#Building models\n",
        "\n",
        "In this part, two models will be build, to test which one is better. A LSTM model will be build, aswell as a GRU model. The GRU is similar to the LSTM, but according to this paper from Cornell, it doesn't perform as good as LSTM (https://arxiv.org/abs/1805.04908). Lets test it on our own. How fun.\n",
        "\n",
        "The models are build with one layer of LSTM or GRU, followed by two Dense layers with a dropout-layer in between.\n",
        "\n",
        "The dropout layers are put there as a measure to hopefully prevent overfitting the data. While it might be an exaggeration to put two layers in there, I chose to do so to be sure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALO1nX-BVfpq",
        "outputId": "5b70b20b-a359-4bb2-e678-ba5066a3494a"
      },
      "source": [
        "#The LSTM Model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(input_dim = max_words, output_dim = 128))\n",
        "lstm_model.add(LSTM(units = 128, dropout = 0.2, recurrent_dropout = 0.2))\n",
        "lstm_model.add(Dropout(rate = 0.5))\n",
        "lstm_model.add(Dense(units = 128, activation = 'relu'))\n",
        "lstm_model.add(Dropout(rate = 0.5))\n",
        "lstm_model.add(Dense(1, activation = 'sigmoid'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoM4cuK5V_-0",
        "outputId": "3bdc3217-f84f-4c7e-fa77-a315da5dea2f"
      },
      "source": [
        "lstm_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_17 (Embedding)     (None, None, 128)         37888     \n",
            "_________________________________________________________________\n",
            "lstm_17 (LSTM)               (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 186,113\n",
            "Trainable params: 186,113\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAPVEEWsV_6i"
      },
      "source": [
        "#Compiling the LSTM model.\n",
        "lstm_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9WaAVDeartB",
        "outputId": "d951bf06-b9fd-413a-a1ae-b66b8ce84c3c"
      },
      "source": [
        "# #Fitting the LSTM model.\n",
        "lstm_model.fit(x_train, y_train, batch_size=32, epochs=4, validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "287/287 [==============================] - 438s 2s/step - loss: 0.4344 - accuracy: 0.8068 - val_loss: 0.3616 - val_accuracy: 0.8412\n",
            "Epoch 2/4\n",
            "287/287 [==============================] - 431s 2s/step - loss: 0.3531 - accuracy: 0.8545 - val_loss: 0.3518 - val_accuracy: 0.8530\n",
            "Epoch 3/4\n",
            "287/287 [==============================] - 427s 1s/step - loss: 0.3295 - accuracy: 0.8645 - val_loss: 0.3410 - val_accuracy: 0.8599\n",
            "Epoch 4/4\n",
            "287/287 [==============================] - 428s 1s/step - loss: 0.3104 - accuracy: 0.8690 - val_loss: 0.3271 - val_accuracy: 0.8639\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7faf6ba85080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 501
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI4pk3tgaXRr",
        "outputId": "56ee254a-7119-458a-cf11-9db828c5f0e4"
      },
      "source": [
        "#The GRU model.\n",
        "gru_model = Sequential()\n",
        "gru_model.add(Embedding(input_dim = max_words, output_dim = 128))\n",
        "gru_model.add(GRU(units = 128, dropout = 0.2, recurrent_dropout = 0.2))\n",
        "gru_model.add(Dropout(rate = 0.5))\n",
        "gru_model.add(Dense(units = 128, activation = 'relu'))\n",
        "gru_model.add(Dropout(rate = 0.5))\n",
        "gru_model.add(Dense(1, activation = 'sigmoid'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiC0Ti7ya5li",
        "outputId": "8c8882e0-90fe-4aff-d99d-35d6aa7d0bbd"
      },
      "source": [
        "gru_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_18 (Embedding)     (None, None, 128)         37888     \n",
            "_________________________________________________________________\n",
            "gru_5 (GRU)                  (None, 128)               99072     \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 153,601\n",
            "Trainable params: 153,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqVWQ5mearSS"
      },
      "source": [
        "#Compiling the GRU model.\n",
        "gru_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "3h7OcLYRauyL",
        "outputId": "9e7257b0-512b-4952-8945-c3ea73864d90"
      },
      "source": [
        "#Fitting the GRU model.\n",
        "gru_model.fit(x_train, y_train, batch_size=32, epochs=4, validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "287/287 [==============================] - 386s 1s/step - loss: 0.4774 - accuracy: 0.7631 - val_loss: 0.3604 - val_accuracy: 0.8560\n",
            "Epoch 2/4\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.3560 - accuracy: 0.8511"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-505-8b09589c09f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Fitting the GRU model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgru_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m   1134\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TraceContext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6okuRRB0biIr"
      },
      "source": [
        "#What do we do from here?\n",
        "\n",
        "Lets add the prediction from each model onto our testing data-set, where we have a whole \"new\" data-set, with new tweets. The we can show each models prediction, alongside the label. This will give us a true-score as we can clearly see which model gets it correct most times.\n",
        "\n",
        "To do so, we must first tokenize the words in the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HzmoDeFwjNp"
      },
      "source": [
        "tokenizer.fit_on_texts(test['tweet'])\n",
        "test_sequences = tokenizer.texts_to_sequences(test['tweet'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XmeMPrlwjNp"
      },
      "source": [
        "#Now we need to pad the tweets, to make them all equal length\n",
        "test_sequences = pad_sequences(sequences = test_sequences, maxlen = max_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMmQhp1_wjNp"
      },
      "source": [
        "#Then we create new arrays for each models prediction. \n",
        "lstm_prediction = lstm_model.predict_classes(test_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tv4l7Xxw5L9"
      },
      "source": [
        "gru_prediction = gru_model.predict_classes(test_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAQgeCXazHj8"
      },
      "source": [
        "#Then we turn the numpy arrays into dataframes\n",
        "lstm_prediction = pd.DataFrame(lstm_prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is3tFE6wzC1U"
      },
      "source": [
        "gru_prediction = pd.DataFrame(gru_prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr-e6G0QzONn"
      },
      "source": [
        "#Now we merge the labels together, to keep it all in one place.\n",
        "final_predictions = lstm_prediction.merge(gru_prediction, how='left', left_index=True, right_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WPk41_8z0cF"
      },
      "source": [
        "#We rename the columns for prettyness :-)\n",
        "final_predictions.columns = ['LSTM_pred','GRU_pred']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYOPEUAK2P6l"
      },
      "source": [
        "#Merging the it all together\n",
        "test = test.merge(final_predictions, how='left', left_index=True, right_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3pCNvnTEXVn"
      },
      "source": [
        "#Creating a mapping to replace 0 and 1 with False and True. \n",
        "cleanup_nums = {\"LSTM_pred\":     {0: 'False', 1: 'True'},\n",
        "                \"GRU_pred\":     {0: 'False', 1: 'True'}\n",
        "                }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1rRWy7LE1uQ"
      },
      "source": [
        "#And replacing values with True/False\n",
        "test.replace(cleanup_nums, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycfhUHlrVjCs"
      },
      "source": [
        "#Short analysis\n",
        "Now we get to the fun part. Even though the task originally was to create a model that could determine if a tweet was Trumps or not, we have also put it upon ourselves to make a mini-test if the GRU was worse than the LSTM. \n",
        "\n",
        "To do so, we cross-tab each prediction with the correct prediction, and with each other. This way we can compare their results. In some way, it works the same with students and exams. First we compare the two students with the correct exam-answers, and afterwords we compare how often the two students disagree on a question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsx55M8zGbS3"
      },
      "source": [
        "pd.crosstab(test.label, test.LSTM_pred, normalize=\"columns\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH-atZn_Gga6"
      },
      "source": [
        "pd.crosstab(test.label, test.GRU_pred, normalize=\"columns\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkprIMwwGgvh"
      },
      "source": [
        "pd.crosstab(test.GRU_pred, test.LSTM_pred, normalize=\"columns\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j33QIAFJWcKG"
      },
      "source": [
        "#The big wrap-up\n",
        "\n",
        "From the crosstabs we get that, the LSTM is correct 76,8% of the time (False/False + True/True divided by 2) and the GRU model is correct 75,4% of the time. This however, doesn't necessarily mean that the GRU model is the worst of those, because that number is influenced by how often the model has predicted a false tweet to be false. If we only wanted a model that could predict the tweets right, the GRU actually performs sligthly better than the LSTM model.\n",
        "\n",
        "However, as the LSTM is more correct in general, I would choose that one, but you cannot dismiss the GRU model aswell. Interestingly enough, the models agree on the tweets in 93% of the times. In theory we could go even further down the rabit-hole and explore what the tweets that they disagree on are, and perhaps do some NLP, to look for topics, or what-not. Unfortunately, that is for another time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yvXqZLknTDm"
      },
      "source": [
        "Link to the colab:\n",
        "\n",
        "https://bit.ly/3mrwHp9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-OABKSTnF5I"
      },
      "source": [
        "!jupyter nbconvert --to html \"/content/M3A1.ipynb\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}