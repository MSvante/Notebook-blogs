{
  
    
        "post0": {
            "title": "Amazon",
            "content": "Preproccesing . !pip install yfinance . Requirement already satisfied: yfinance in /usr/local/lib/python3.6/dist-packages (0.1.55) Requirement already satisfied: requests&gt;=2.20 in /usr/local/lib/python3.6/dist-packages (from yfinance) (2.23.0) Requirement already satisfied: pandas&gt;=0.24 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.1.5) Requirement already satisfied: lxml&gt;=4.5.1 in /usr/local/lib/python3.6/dist-packages (from yfinance) (4.6.2) Requirement already satisfied: multitasking&gt;=0.0.7 in /usr/local/lib/python3.6/dist-packages (from yfinance) (0.0.9) Requirement already satisfied: numpy&gt;=1.15 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.19.4) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.20-&gt;yfinance) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2020.12.5) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.20-&gt;yfinance) (1.24.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2.10) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2.8.1) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.24-&gt;yfinance) (1.15.0) . !pip3 install --user --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint . Collecting twint Cloning https://github.com/twintproject/twint.git (to revision origin/master) to /tmp/pip-install-p8nbyt5e/twint Running command git clone -q https://github.com/twintproject/twint.git /tmp/pip-install-p8nbyt5e/twint WARNING: Did not find branch or tag &#39;origin/master&#39;, assuming revision or ref. Running command git checkout -q origin/master Requirement already satisfied, skipping upgrade: aiohttp in /root/.local/lib/python3.6/site-packages (from twint) (3.7.3) Requirement already satisfied, skipping upgrade: aiodns in /root/.local/lib/python3.6/site-packages (from twint) (2.0.0) Requirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from twint) (4.6.3) Requirement already satisfied, skipping upgrade: cchardet in /root/.local/lib/python3.6/site-packages (from twint) (2.1.7) Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from twint) (0.8) Requirement already satisfied, skipping upgrade: elasticsearch in /root/.local/lib/python3.6/site-packages (from twint) (7.10.1) Requirement already satisfied, skipping upgrade: pysocks in /usr/local/lib/python3.6/dist-packages (from twint) (1.7.1) Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from twint) (1.1.5) Requirement already satisfied, skipping upgrade: aiohttp_socks in /root/.local/lib/python3.6/site-packages (from twint) (0.5.5) Requirement already satisfied, skipping upgrade: schedule in /root/.local/lib/python3.6/site-packages (from twint) (0.6.0) Requirement already satisfied, skipping upgrade: geopy in /usr/local/lib/python3.6/dist-packages (from twint) (1.17.0) Requirement already satisfied, skipping upgrade: fake-useragent in /root/.local/lib/python3.6/site-packages (from twint) (0.1.11) Requirement already satisfied, skipping upgrade: googletransx in /root/.local/lib/python3.6/site-packages (from twint) (2.4.2) Requirement already satisfied, skipping upgrade: chardet&lt;4.0,&gt;=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp-&gt;twint) (3.0.4) Requirement already satisfied, skipping upgrade: multidict&lt;7.0,&gt;=4.5 in /root/.local/lib/python3.6/site-packages (from aiohttp-&gt;twint) (5.1.0) Requirement already satisfied, skipping upgrade: async-timeout&lt;4.0,&gt;=3.0 in /root/.local/lib/python3.6/site-packages (from aiohttp-&gt;twint) (3.0.1) Requirement already satisfied, skipping upgrade: attrs&gt;=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp-&gt;twint) (20.3.0) Requirement already satisfied, skipping upgrade: yarl&lt;2.0,&gt;=1.0 in /root/.local/lib/python3.6/site-packages (from aiohttp-&gt;twint) (1.6.3) Requirement already satisfied, skipping upgrade: idna-ssl&gt;=1.0; python_version &lt; &#34;3.7&#34; in /root/.local/lib/python3.6/site-packages (from aiohttp-&gt;twint) (1.1.0) Requirement already satisfied, skipping upgrade: typing-extensions&gt;=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp-&gt;twint) (3.7.4.3) Requirement already satisfied, skipping upgrade: typing; python_version &lt; &#34;3.7&#34; in /root/.local/lib/python3.6/site-packages (from aiodns-&gt;twint) (3.7.4.3) Requirement already satisfied, skipping upgrade: pycares&gt;=3.0.0 in /root/.local/lib/python3.6/site-packages (from aiodns-&gt;twint) (3.1.1) Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.6/dist-packages (from elasticsearch-&gt;twint) (2020.12.5) Requirement already satisfied, skipping upgrade: urllib3&lt;2,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch-&gt;twint) (1.24.3) Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;twint) (2.8.1) Requirement already satisfied, skipping upgrade: numpy&gt;=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;twint) (1.19.4) Requirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;twint) (2018.9) Requirement already satisfied, skipping upgrade: python-socks[asyncio]&gt;=1.0.1 in /root/.local/lib/python3.6/site-packages (from aiohttp_socks-&gt;twint) (1.1.2) Requirement already satisfied, skipping upgrade: geographiclib&lt;2,&gt;=1.49 in /usr/local/lib/python3.6/dist-packages (from geopy-&gt;twint) (1.50) Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from googletransx-&gt;twint) (2.23.0) Requirement already satisfied, skipping upgrade: idna&gt;=2.0 in /usr/local/lib/python3.6/dist-packages (from yarl&lt;2.0,&gt;=1.0-&gt;aiohttp-&gt;twint) (2.10) Requirement already satisfied, skipping upgrade: cffi&gt;=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pycares&gt;=3.0.0-&gt;aiodns-&gt;twint) (1.14.4) Requirement already satisfied, skipping upgrade: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;twint) (1.15.0) Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi&gt;=1.5.0-&gt;pycares&gt;=3.0.0-&gt;aiodns-&gt;twint) (2.20) Building wheels for collected packages: twint Building wheel for twint (setup.py) ... done Created wheel for twint: filename=twint-2.1.21-cp36-none-any.whl size=38761 sha256=8e0ca3f7b4f2b6047a1e9b3c35f64d54664c30bbaaefeff1d95b6045d28aea9d Stored in directory: /tmp/pip-ephem-wheel-cache-ynrgregn/wheels/4f/3b/75/62d04b3b446658ba85401e8868d3cd1d4bc22f17ad755460a6 Successfully built twint Installing collected packages: twint Found existing installation: twint 2.1.21 Uninstalling twint-2.1.21: Successfully uninstalled twint-2.1.21 WARNING: The script twint is installed in &#39;/root/.local/bin&#39; which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed twint-2.1.21 . import pandas as pd from pandas_datareader import data as web from yfinance import Ticker import datetime import yfinance as yf import keras from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM from keras.layers import Dropout import numpy as np from keras.preprocessing.sequence import TimeseriesGenerator from sklearn.preprocessing import MinMaxScaler import matplotlib.pyplot as plt . import twint import nest_asyncio nest_asyncio.apply() import nltk from nltk.corpus import stopwords from nltk.stem import WordNetLemmatizer nltk.download(&#39;stopwords&#39;) nltk.download(&#39;wordnet&#39;) from textblob import TextBlob . [nltk_data] Downloading package stopwords to /root/nltk_data... [nltk_data] Unzipping corpora/stopwords.zip. [nltk_data] Downloading package wordnet to /root/nltk_data... [nltk_data] Unzipping corpora/wordnet.zip. . start_date = (datetime.datetime(2020,12,21) - datetime.timedelta(days=1825)).strftime(&quot;%Y-%m-%d&quot;) end_date = datetime.datetime(2020,12,21).strftime(&quot;%Y-%m-%d&quot;) . lemmatizer = WordNetLemmatizer() stop_words = set(stopwords.words(&#39;english&#39;)) def sneaky_cleanup(title): tokens = [] for token in title.split(): if token not in stop_words: if token.isalnum(): tokens.append(lemmatizer.lemmatize(token)) return &quot; &quot;.join(tokens) . stock = &quot;Amazon&quot; . #Configuration c = twint.Config() c.Username = (&quot;CNBC&quot;) c.Search = stock c.Since = &quot;2015-1-1&quot; c.Count = True c.Filter_retweets = True c.Pandas = True . twint.run.Search(c) . df = twint.storage.panda.Tweets_df . df_cnbc_amzn = df[[&#39;id&#39;,&#39;date&#39;,&#39;tweet&#39;,&#39;hashtags&#39;,&#39;username&#39;,&#39;search&#39;]] df_cnbc_amzn.head() . id date tweet hashtags username search . 0 1345857469533319169 | 2021-01-03 22:20:00 | .@dee_bosa breaks down the response to the str... | [] | CNBC | Amazon | . 1 1345698164033134592 | 2021-01-03 11:46:58 | Jeff Bezos&#39; 3-question rule for hiring new Ama... | [] | CNBC | Amazon | . 2 1345492816714055681 | 2021-01-02 22:11:00 | .@dee_bosa breaks down the response to the str... | [] | CNBC | Amazon | . 3 1345328999317336064 | 2021-01-02 11:20:03 | You got an Amazon Echo for the holidays — @rob... | [] | CNBC | Amazon | . 4 1345277950690734080 | 2021-01-02 07:57:12 | Amazon is filled with fake reviews and it&#39;s ge... | [] | CNBC | Amazon | . df_cnbc_amzn.tweet = df_cnbc_amzn.tweet.apply(lambda x: sneaky_cleanup(x)) . /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . def polarity(text): return TextBlob(text).sentiment.polarity . df_cnbc_amzn[&quot;polarity&quot;] = df_cnbc_amzn[&quot;tweet&quot;].apply(polarity) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . df_cnbc_amzn.head() . id date tweet hashtags username search polarity . 0 1344003481628266499 | 2020-12-29 19:32:55 | break response NFL game San Francisco 49ers Ar... | [] | CNBC | Amazon | -0.4 | . 1 1343959912343334914 | 2020-12-29 16:39:47 | Stick winner Microsoft drive market two trader... | [] | CNBC | Amazon | 0.0 | . 2 1342914094538567680 | 2020-12-26 19:24:04 | Amazon hire lobbyist brother Biden White House... | [] | CNBC | Amazon | 0.0 | . 3 1342179425513840640 | 2020-12-24 18:44:46 | expect Google compete Amazon anytime soon | [] | CNBC | Amazon | 0.0 | . 4 1341877532761665541 | 2020-12-23 22:45:09 | Amazon warehouse worker could vote unionize ne... | [] | CNBC | Amazon | 0.0 | . #Configuration c = twint.Config() c.Username = (&quot;business&quot;) c.Search = stock c.Since = &quot;2015-1-1&quot; c.Count = True c.Filter_retweets = True c.Pandas = True . twint.run.Search(c) . df = twint.storage.panda.Tweets_df . df_bloom = df[[&#39;id&#39;,&#39;date&#39;,&#39;tweet&#39;,&#39;hashtags&#39;,&#39;username&#39;,&#39;search&#39;]] df_bloom . id date tweet hashtags username search . 0 1345769903077388290 | 2021-01-03 16:32:02 | Amazon self-driving startup Zoox unveils a ful... | [] | business | Amazon | . 1 1345513716939427840 | 2021-01-02 23:34:03 | Many Amazon warehouse employees struggle to pa... | [] | business | Amazon | . 2 1345037069224206336 | 2021-01-01 16:00:01 | Many Amazon warehouse employees struggle to pa... | [] | business | Amazon | . 3 1344364405878890500 | 2020-12-30 19:27:06 | Amazon agreed to acquire podcasting company Wo... | [] | business | Amazon | . 4 1344306980031242241 | 2020-12-30 15:38:54 | The pandemic sped up the rush to online shoppi... | [] | business | Amazon | . ... ... | ... | ... | ... | ... | ... | . 4275 556254622160273409 | 2015-01-17 01:00:11 | These are the Amazon Prime pilots getting the ... | [] | business | Amazon | . 4276 555167752865218560 | 2015-01-14 01:01:21 | Amazon just signed Woody Allen to his first TV... | [] | business | Amazon | . 4277 555011593785077760 | 2015-01-13 14:40:50 | BREAKING: Amazon enlists Woody Allen to create... | [] | business | Amazon | . 4278 553929462535176194 | 2015-01-10 15:00:50 | Amazon&#39;s instant refund aims to get you spendi... | [] | business | Amazon | . 4279 552104052486840320 | 2015-01-05 14:07:18 | Amazon reps are taking over CES, and Apple is ... | [] | business | Amazon | . 4280 rows × 6 columns . df_bloom.tweet = df_bloom.tweet.apply(lambda x: sneaky_cleanup(x)) . /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . df_bloom[&quot;polarity&quot;] = df_bloom[&quot;tweet&quot;].apply(polarity) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . #Configuration c = twint.Config() c.Username = (&quot;WSJ&quot;) c.Search = stock c.Since = &quot;2015-1-1&quot; c.Count = True c.Filter_retweets = True c.Pandas = True . twint.run.Search(c) . df = twint.storage.panda.Tweets_df . df_wsj = df[[&#39;id&#39;,&#39;date&#39;,&#39;tweet&#39;,&#39;hashtags&#39;,&#39;username&#39;,&#39;search&#39;]] . df_wsj.tweet = df_wsj.tweet.apply(lambda x: sneaky_cleanup(x)) . /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . df_wsj[&quot;polarity&quot;] = df_wsj[&quot;tweet&quot;].apply(polarity) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . frames=[df_cnbc_amzn,df_bloom,df_wsj] . merged = pd.concat(frames) . merged[&quot;date&quot;] = pd.to_datetime(merged.date) merged[&quot;date&quot;] = merged[&quot;date&quot;] + datetime.timedelta(hours=8) . positive = merged[merged[&quot;polarity&quot;]&gt;0] neutral = merged[merged[&quot;polarity&quot;]==0] negative = merged[merged[&quot;polarity&quot;]&lt;0] . positive = positive.set_index(&#39;date&#39;).resample(&#39;D&#39;)[&#39;polarity&#39;].count() neutral = neutral.set_index(&#39;date&#39;).resample(&#39;D&#39;)[&#39;polarity&#39;].count() negative = negative.set_index(&#39;date&#39;).resample(&#39;D&#39;)[&#39;polarity&#39;].count() . positive = pd.DataFrame(positive) neutral = pd.DataFrame(neutral) negative = pd.DataFrame(negative) . positive[&quot;positive&quot;] = positive.polarity neutral[&quot;neutral&quot;]=neutral.polarity negative[&quot;negative&quot;]=negative.polarity . df2 = pd.merge(positive,neutral,left_index=True,right_index=True) . sentiment = pd.merge(df2,negative,right_index=True,left_index=True) . df = yf.download(&quot;AMZN&quot;, start=start_date, end=end_date, progress=False, interval=&#39;1d&#39;) . df.columns = [w.lower() for w in df.columns] . df = pd.merge(df,sentiment,left_index=True,right_index=True) . df[&#39;close-1&#39;] = df[&#39;close&#39;].shift(+1, fill_value=df[&#39;close&#39;].iloc[0]) . df[&#39;change&#39;] = df[&quot;close&quot;] - df[&#39;close&#39;].shift(+1, fill_value=df[&#39;close&#39;].iloc[0]) #df[&#39;change_pred&#39;] = (df[&quot;close&quot;] - df[&#39;close&#39;].shift(-1, fill_value=df[&#39;change&#39;].iloc[1]))*-1 . df = df.drop([&quot;open&quot;,&quot;polarity_x&quot;,&quot;polarity_y&quot;,&quot;polarity&quot;,&quot;adj close&quot;,&quot;close&quot;,&quot;high&quot;,&quot;low&quot;],axis=1) . . Since we want to predict the closing stock price for the following day, we just shift the closing price one day to get our y value. . Since we are working with sequential data, we dont use train_test_split. Insted we pick the first 80% of obersavations as training set, and the remaning as testing set. . test_size = int(len(df) * 0.1) train = df.iloc[:-test_size,:].copy() test = df.iloc[-test_size:,:].copy() . We split the dataset into x values and y values. We also specify .values, since the date is not relevant for the training and dosn&#39;t work with some of the later preprocessing . X_train = train.iloc[:,:-1].values y_train = train.iloc[:,-1].values X_test = test.iloc[:,:-1].values y_test = test.iloc[:,-1].values . We scale all our values to be between -1 and 1. This should help the accuracy of the model. . x_scaler = MinMaxScaler(feature_range=(-1, 1)) y_scaler = MinMaxScaler(feature_range=(-1, 1)) . X_train = x_scaler.fit_transform(X_train) y_train = y_scaler.fit_transform(y_train.reshape(-1,1)) X_test = x_scaler.transform(X_test) y_test = y_scaler.transform(y_test.reshape(-1,1)) . X_train = np.reshape(X_train, (X_train.shape[0], 1, 5)) X_test = np.reshape(X_test, (X_test.shape[0], 1, 5)) . Now we start making our RNN model. . n_input = how many days we look in the past to predict the next sample. We chose 20 mostly by trial and error. . We set epochs to 100. It&#39;s our experience that the more you train the model, the more it will try to predict the daily volatility. If we only trained it for eg. 10 epochs, the model would make a soft curve which didn&#39;t look like a real stock development. By trail and error we found 100 to be the best training amount. . n_input = 10 n_features= X_train.shape[2] # how many predictors/Xs/features we have to predict y b_size = 10 # Number of timeseries samples in each batch epochs = 200 . Since we are working with sequential stock data we chose an LSTM model, which is a RNN model. Activation function is set to relu, and optimizer is adam, since these are the standard for this kind of task. We chose 2 layers with 50 units each, and once again done by trial and error. . Since we want to predict the actual stock price we use mse(mean squared error) as our loss fuction. . model = Sequential() model.add(LSTM(50, activation=&#39;relu&#39;,return_sequences=True, input_shape=(n_input, n_features))) model.add(Dropout(0.2)) model.add(LSTM(50, activation=&#39;relu&#39;)) model.add(Dense(1)) model.compile(optimizer=&#39;adam&#39;, loss=&#39;MSE&#39;,metrics=[&#39;MSE&#39;]) model.summary() . Model: &#34;sequential_12&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm_24 (LSTM) (None, 10, 50) 11200 _________________________________________________________________ dropout_12 (Dropout) (None, 10, 50) 0 _________________________________________________________________ lstm_25 (LSTM) (None, 50) 20200 _________________________________________________________________ dense_12 (Dense) (None, 1) 51 ================================================================= Total params: 31,451 Trainable params: 31,451 Non-trainable params: 0 _________________________________________________________________ . model.fit(X_train,y_train,epochs=epochs,verbose=1) . import matplotlib.pyplot as plt . This looks like overfitting, but it works the best in our case. . loss_per_epoch = model.history.history[&#39;loss&#39;] plt.plot(range(len(loss_per_epoch)),loss_per_epoch); . Then we do some data processing to get it back to the orginal format, so we can compare the real data to the predictions. . y_pred_scaled = model.predict(X_test) y_pred = y_scaler.inverse_transform(y_pred_scaled) y_test = y_scaler.inverse_transform(y_test) results = pd.DataFrame({&#39;y_true&#39;:y_test.flatten(),&#39;y_pred&#39;:y_pred.flatten()}) print(results) . WARNING:tensorflow:Model was constructed with shape (None, 10, 5) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10, 5), dtype=tf.float32, name=&#39;lstm_24_input&#39;), name=&#39;lstm_24_input&#39;, description=&#34;created by layer &#39;lstm_24_input&#39;&#34;), but it was called on an input with incompatible shape (None, 1, 5). . WARNING:tensorflow:Model was constructed with shape (None, 10, 5) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10, 5), dtype=tf.float32, name=&#39;lstm_24_input&#39;), name=&#39;lstm_24_input&#39;, description=&#34;created by layer &#39;lstm_24_input&#39;&#34;), but it was called on an input with incompatible shape (None, 1, 5). . y_true y_pred 0 -30.010010 26.114405 1 20.180176 27.126236 2 -61.709961 -17.084387 3 -12.490234 7.946451 4 78.440186 27.488426 .. ... ... 120 40.550049 12.756760 121 8.150146 -39.441650 122 75.839844 -46.192448 123 -4.879883 -73.570068 124 -34.430176 -155.441513 [125 rows x 2 columns] . results[&quot;y_pred&quot;] = (results[&quot;y_pred&quot;]&gt;0).astype(int).astype(&quot;float32&quot;) results[&quot;y_true&quot;] = (results[&quot;y_true&quot;]&gt;0).astype(int).astype(&quot;float32&quot;) . backtesting = pd.merge(test.reset_index(),results,right_index=True,left_index=True) . buy = backtesting[backtesting[&quot;y_pred&quot;]==1] sell = backtesting[backtesting[&quot;y_pred&quot;]==0] . pd.DataFrame(backtesting.sort_values(by=[&quot;change&quot;],ascending=False).iloc[0:10,:]) . index volume positive neutral negative close-1 change y_true y_pred . 17 2020-07-20 | 7598200 | 2 | 1 | 0 | 2961.969971 | 234.870117 | 1.0 | 0.0 | . 93 2020-11-04 | 6839000 | 0 | 0 | 0 | 3048.409912 | 192.750000 | 1.0 | 0.0 | . 62 2020-09-22 | 6948800 | 6 | 4 | 0 | 2960.469971 | 168.520020 | 1.0 | 0.0 | . 7 2020-07-06 | 6880600 | 3 | 0 | 0 | 2890.300049 | 166.739990 | 1.0 | 1.0 | . 76 2020-10-12 | 8364200 | 1 | 1 | 1 | 3286.649902 | 156.280029 | 1.0 | 0.0 | . 38 2020-08-18 | 5346000 | 2 | 9 | 1 | 3182.409912 | 130.080078 | 1.0 | 0.0 | . 5 2020-07-01 | 6363400 | 3 | 0 | 1 | 2758.820068 | 119.879883 | 1.0 | 1.0 | . 53 2020-09-09 | 5188700 | 0 | 0 | 2 | 3149.840088 | 118.770020 | 1.0 | 0.0 | . 26 2020-07-31 | 8085500 | 6 | 13 | 1 | 3051.879883 | 112.800049 | 1.0 | 0.0 | . 98 2020-11-11 | 4366900 | 2 | 1 | 5 | 3035.020020 | 102.369873 | 1.0 | 0.0 | . pd.DataFrame(backtesting.sort_values(by=[&quot;change&quot;],ascending=True).iloc[0:10,:],) . index volume positive neutral negative close-1 change y_true y_pred . 90 2020-10-30 | 8386400 | 3 | 12 | 5 | 3211.010010 | -174.860107 | 0.0 | 0.0 | . 96 2020-11-09 | 7190400 | 1 | 1 | 0 | 3311.370117 | -167.630127 | 0.0 | 0.0 | . 50 2020-09-03 | 8161100 | 2 | 0 | 0 | 3531.449951 | -163.449951 | 0.0 | 0.0 | . 52 2020-09-08 | 6094200 | 1 | 0 | 3 | 3294.620117 | -144.780029 | 0.0 | 0.0 | . 63 2020-09-23 | 5652700 | 3 | 9 | 1 | 3128.989990 | -129.129883 | 0.0 | 0.0 | . 88 2020-10-28 | 5588300 | 3 | 1 | 1 | 3286.330078 | -123.550049 | 0.0 | 0.0 | . 20 2020-07-23 | 5656900 | 3 | 2 | 0 | 3099.909912 | -113.359863 | 0.0 | 0.0 | . 97 2020-11-10 | 6591000 | 1 | 2 | 2 | 3143.739990 | -108.719971 | 0.0 | 0.0 | . 72 2020-10-06 | 5086900 | 0 | 1 | 1 | 3199.199951 | -99.239990 | 0.0 | 0.0 | . 70 2020-10-02 | 5613100 | 4 | 3 | 0 | 3221.260010 | -96.260010 | 0.0 | 0.0 | . gain = buy.change.sum() loss = sell.change.sum() . profit = gain - loss . profit . 739.820068359375 . pd.crosstab(results.y_true,results.y_pred) . y_pred 0.0 1.0 . y_true . 0.0 49 | 12 | . 1.0 47 | 17 | . test=df[df.positive/df.negative&gt;5] . test.change.mean() . 3.987776219844818 . test2=df[df.negative/df.positive&gt;5] . test2.change.mean() . 1.7992342435396635 . test2 . volume positive neutral negative close-1 change . 2016-02-04 6199100 | 0 | 0 | 1 | 531.070007 | 5.190002 | . 2016-02-22 5566600 | 0 | 0 | 1 | 534.900024 | 24.599976 | . 2016-03-08 4730000 | 0 | 0 | 2 | 562.799988 | -2.539978 | . 2016-04-01 2917400 | 0 | 2 | 1 | 593.640015 | 4.859985 | . 2016-04-13 4228300 | 0 | 3 | 1 | 603.169983 | 11.650024 | . ... ... | ... | ... | ... | ... | ... | . 2020-09-09 5188700 | 0 | 0 | 2 | 3149.840088 | 118.770020 | . 2020-10-06 5086900 | 0 | 1 | 1 | 3199.199951 | -99.239990 | . 2020-11-03 4897900 | 0 | 1 | 2 | 3004.479980 | 43.929932 | . 2020-11-30 4063900 | 0 | 0 | 3 | 3195.340088 | -27.300049 | . 2020-12-07 2751300 | 0 | 0 | 2 | 3162.580078 | -4.580078 | . 78 rows × 6 columns . plt.bar(list(df.iloc[:,1:4].columns.values),df.iloc[:,1:4].sum()) . &lt;BarContainer object of 3 artists&gt; . amzn = df . Apple . stock = &quot;Apple&quot; . #Configuration c = twint.Config() c.Username = (&quot;CNBC&quot;) c.Search = stock c.Since = &quot;2015-1-1&quot; c.Count = True c.Filter_retweets = True c.Pandas = True . twint.run.Search(c) . df = twint.storage.panda.Tweets_df . df_cnbc = df[[&#39;id&#39;,&#39;date&#39;,&#39;tweet&#39;,&#39;hashtags&#39;,&#39;username&#39;,&#39;search&#39;]] df_cnbc.head() . id date tweet hashtags username search . 0 1346095039328481280 | 2021-01-04 14:04:01 | Why Apple became a $2 trillion company in 2020... | [] | CNBC | Apple | . 1 1345890186400583681 | 2021-01-04 00:30:00 | These married former Apple execs used lessons ... | [] | CNBC | Apple | . 2 1345681045862162432 | 2021-01-03 10:38:57 | Elon Musk says he once tried to sell Tesla to ... | [] | CNBC | Apple | . 3 1345646915170480128 | 2021-01-03 08:23:20 | These married former Apple execs used lessons ... | [] | CNBC | Apple | . 4 1345507500456607752 | 2021-01-02 23:09:21 | These married former Apple execs used lessons ... | [] | CNBC | Apple | . df_cnbc.tweet = df_cnbc.tweet.apply(lambda x: sneaky_cleanup(x)) . /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . def polarity(text): return TextBlob(text).sentiment.polarity . df_cnbc[&quot;polarity&quot;] = df_cnbc[&quot;tweet&quot;].apply(polarity) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . df_cnbc.head() . id date tweet hashtags username search polarity . 0 1343978847335821319 | 2020-12-29 17:55:01 | New Here Michael top 10 stock including Check | [] | CNBC | Apple | 0.318182 | . 1 1343959912343334914 | 2020-12-29 16:39:47 | Stick winner Microsoft drive market two trader... | [] | CNBC | Apple | 0.000000 | . 2 1343931413893156864 | 2020-12-29 14:46:32 | These married former Apple exec used lesson le... | [] | CNBC | Apple | 0.000000 | . 3 1343558075211505665 | 2020-12-28 14:03:01 | Top Apple analyst pours cold water latest Appl... | [] | CNBC | Apple | 0.133333 | . 4 1342138013703434240 | 2020-12-24 16:00:12 | Stocks making biggest move | [] | CNBC | Apple | 0.000000 | . #Configuration c = twint.Config() c.Username = (&quot;business&quot;) c.Search = stock c.Since = &quot;2015-1-1&quot; c.Count = True c.Filter_retweets = True c.Pandas = True . twint.run.Search(c) . df = twint.storage.panda.Tweets_df . df_bloom = df[[&#39;id&#39;,&#39;date&#39;,&#39;tweet&#39;,&#39;hashtags&#39;,&#39;username&#39;,&#39;search&#39;]] df_bloom . id date tweet hashtags username search . 0 1344658083763650561 | 2020-12-31 14:54:04 | Apple shares rose, suggesting it will end a ye... | [] | business | Apple | . 1 1344215908085092352 | 2020-12-30 09:37:01 | Apple lost its copyright claims against a Flor... | [] | business | Apple | . 2 1344132886283362310 | 2020-12-30 04:07:07 | Apple lost its copyright claims against a Flor... | [] | business | Apple | . 3 1344025168142860291 | 2020-12-29 20:59:05 | Apple lost its copyright claims against a Flor... | [] | business | Apple | . 4 1343944868608016387 | 2020-12-29 15:40:00 | Apple shares rallied to an intraday record on ... | [] | business | Apple | . ... ... | ... | ... | ... | ... | ... | . 5247 555085364332093440 | 2015-01-13 19:33:58 | BREAKING: GoPro plunges after Apple gains remo... | [] | business | Apple | . 5248 553253586545934337 | 2015-01-08 18:15:08 | Apple says the company has created or supporte... | [] | business | Apple | . 5249 552994362595241984 | 2015-01-08 01:05:04 | Xiaomi’s buying spree gives Apple and Samsung ... | [] | business | Apple | . 5250 552104052486840320 | 2015-01-05 14:07:18 | Amazon reps are taking over CES, and Apple is ... | [] | business | Apple | . 5251 550626425355894785 | 2015-01-01 12:15:44 | Two Apple customers have sued the company for ... | [] | business | Apple | . 5252 rows × 6 columns . df_bloom.tweet = df_bloom.tweet.apply(lambda x: sneaky_cleanup(x)) . /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . df_bloom[&quot;polarity&quot;] = df_bloom[&quot;tweet&quot;].apply(polarity) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . #Configuration c = twint.Config() c.Username = (&quot;WSJ&quot;) c.Search = stock c.Since = &quot;2015-1-1&quot; c.Count = True c.Filter_retweets = True c.Pandas = True . twint.run.Search(c) . df = twint.storage.panda.Tweets_df . df_wsj = df[[&#39;id&#39;,&#39;date&#39;,&#39;tweet&#39;,&#39;hashtags&#39;,&#39;username&#39;,&#39;search&#39;]] . df_wsj.tweet = df_wsj.tweet.apply(lambda x: sneaky_cleanup(x)) . /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . df_wsj[&quot;polarity&quot;] = df_wsj[&quot;tweet&quot;].apply(polarity) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . frames=[df_cnbc,df_bloom,df_wsj] . merged = pd.concat(frames) . merged[&quot;date&quot;] = pd.to_datetime(merged.date) merged[&quot;date&quot;] = merged[&quot;date&quot;] + datetime.timedelta(hours=8) . positive = merged[merged[&quot;polarity&quot;]&gt;0] neutral = merged[merged[&quot;polarity&quot;]==0] negative = merged[merged[&quot;polarity&quot;]&lt;0] . positive = positive.set_index(&#39;date&#39;).resample(&#39;D&#39;)[&#39;polarity&#39;].count() neutral = neutral.set_index(&#39;date&#39;).resample(&#39;D&#39;)[&#39;polarity&#39;].count() negative = negative.set_index(&#39;date&#39;).resample(&#39;D&#39;)[&#39;polarity&#39;].count() . positive = pd.DataFrame(positive) neutral = pd.DataFrame(neutral) negative = pd.DataFrame(negative) . positive[&quot;positive&quot;] = positive.polarity neutral[&quot;neutral&quot;]=neutral.polarity negative[&quot;negative&quot;]=negative.polarity . df2 = pd.merge(positive,neutral,left_index=True,right_index=True) . sentiment = pd.merge(df2,negative,right_index=True,left_index=True) . df = yf.download(&quot;AAPL&quot;, start=start_date, end=end_date, progress=False, interval=&#39;1d&#39;) . df.columns = [w.lower() for w in df.columns] . df = pd.merge(df,sentiment,left_index=True,right_index=True) . df[&#39;close-1&#39;] = df[&#39;close&#39;].shift(+1, fill_value=df[&#39;close&#39;].iloc[0]) . df[&#39;change&#39;] = df[&quot;close&quot;] - df[&#39;close&#39;].shift(+1, fill_value=df[&#39;close&#39;].iloc[0]) #df[&#39;change_pred&#39;] = (df[&quot;close&quot;] - df[&#39;close&#39;].shift(-1, fill_value=df[&#39;change&#39;].iloc[1]))*-1 . df = df.drop([&quot;open&quot;,&quot;polarity_x&quot;,&quot;polarity_y&quot;,&quot;polarity&quot;,&quot;adj close&quot;,&quot;close&quot;,&quot;high&quot;,&quot;low&quot;],axis=1) . . Since we want to predict the closing stock price for the following day, we just shift the closing price one day to get our y value. . Since we are working with sequential data, we dont use train_test_split. Insted we pick the first 80% of obersavations as training set, and the remaning as testing set. . test_size = int(len(df) * 0.1) train = df.iloc[:-test_size,:].copy() test = df.iloc[-test_size:,:].copy() . We split the dataset into x values and y values. We also specify .values, since the date is not relevant for the training and dosn&#39;t work with some of the later preprocessing . X_train = train.iloc[:,:-1].values y_train = train.iloc[:,-1].values X_test = test.iloc[:,:-1].values y_test = test.iloc[:,-1].values . from sklearn.preprocessing import MinMaxScaler . We scale all our values to be between -1 and 1. This should help the accuracy of the model. . x_scaler = MinMaxScaler(feature_range=(-1, 1)) y_scaler = MinMaxScaler(feature_range=(-1, 1)) . X_train = x_scaler.fit_transform(X_train) y_train = y_scaler.fit_transform(y_train.reshape(-1,1)) X_test = x_scaler.transform(X_test) y_test = y_scaler.transform(y_test.reshape(-1,1)) . X_train = np.reshape(X_train, (X_train.shape[0], 1, 5)) X_test = np.reshape(X_test, (X_test.shape[0], 1, 5)) . from keras.preprocessing.sequence import TimeseriesGenerator . Now we start making our RNN model. . n_input = how many days we look in the past to predict the next sample. We chose 20 mostly by trial and error. . We set epochs to 100. It&#39;s our experience that the more you train the model, the more it will try to predict the daily volatility. If we only trained it for eg. 10 epochs, the model would make a soft curve which didn&#39;t look like a real stock development. By trail and error we found 100 to be the best training amount. . n_input = 10 n_features= X_train.shape[2] # how many predictors/Xs/features we have to predict y b_size = 10 # Number of timeseries samples in each batch epochs = 200 . Since we are working with sequential stock data we chose an LSTM model, which is a RNN model. Activation function is set to relu, and optimizer is adam, since these are the standard for this kind of task. We chose 2 layers with 50 units each, and once again done by trial and error. . Since we want to predict the actual stock price we use mse(mean squared error) as our loss fuction. . model = Sequential() model.add(LSTM(50, activation=&#39;relu&#39;,return_sequences=True, input_shape=(n_input, n_features))) model.add(Dropout(0.2)) model.add(LSTM(50, activation=&#39;relu&#39;)) model.add(Dense(1)) model.compile(optimizer=&#39;adam&#39;, loss=&#39;MSE&#39;,metrics=[&#39;MSE&#39;]) model.summary() . Model: &#34;sequential_22&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm_44 (LSTM) (None, 10, 50) 11200 _________________________________________________________________ dropout_22 (Dropout) (None, 10, 50) 0 _________________________________________________________________ lstm_45 (LSTM) (None, 50) 20200 _________________________________________________________________ dense_22 (Dense) (None, 1) 51 ================================================================= Total params: 31,451 Trainable params: 31,451 Non-trainable params: 0 _________________________________________________________________ . model.fit(X_train,y_train,epochs=epochs,verbose=1) . This looks like overfitting, but it works the best in our case. . loss_per_epoch = model.history.history[&#39;loss&#39;] plt.plot(range(len(loss_per_epoch)),loss_per_epoch); . Then we do some data processing to get it back to the orginal format, so we can compare the real data to the predictions. . y_pred_scaled = model.predict(X_test) y_pred = y_scaler.inverse_transform(y_pred_scaled) y_test = y_scaler.inverse_transform(y_test) results = pd.DataFrame({&#39;y_true&#39;:y_test.flatten(),&#39;y_pred&#39;:y_pred.flatten()}) print(results) . WARNING:tensorflow:Model was constructed with shape (None, 10, 5) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10, 5), dtype=tf.float32, name=&#39;lstm_44_input&#39;), name=&#39;lstm_44_input&#39;, description=&#34;created by layer &#39;lstm_44_input&#39;&#34;), but it was called on an input with incompatible shape (None, 1, 5). . WARNING:tensorflow:Model was constructed with shape (None, 10, 5) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10, 5), dtype=tf.float32, name=&#39;lstm_44_input&#39;), name=&#39;lstm_44_input&#39;, description=&#34;created by layer &#39;lstm_44_input&#39;&#34;), but it was called on an input with incompatible shape (None, 1, 5). . y_true y_pred 0 -1.617500 0.765720 1 1.195000 0.823268 2 -2.802498 0.445819 3 2.037498 0.458753 4 0.754997 0.759976 .. ... ... 120 -0.630005 0.573415 121 6.099998 -0.138065 122 -0.070000 0.289773 123 0.889999 0.257777 124 -2.039993 -5.833440 [125 rows x 2 columns] . results[&quot;y_pred&quot;] = (results[&quot;y_pred&quot;]&gt;0).astype(int).astype(&quot;float32&quot;) results[&quot;y_true&quot;] = (results[&quot;y_true&quot;]&gt;0).astype(int).astype(&quot;float32&quot;) . backtesting = pd.merge(test.reset_index(),results,right_index=True,left_index=True) . buy = backtesting[backtesting[&quot;y_pred&quot;]==1] sell = backtesting[backtesting[&quot;y_pred&quot;]==0] . gain = buy.change.sum() loss = sell.change.sum() . profit = gain - loss . profit . 29.352500915527344 . pd.crosstab(results.y_true,results.y_pred) . y_pred 0.0 1.0 . y_true . 0.0 23 | 34 | . 1.0 24 | 44 | . pd.DataFrame(backtesting.sort_values(by=[&quot;change&quot;],ascending=False).iloc[0:10,:]) . index volume positive neutral negative close-1 change y_true y_pred . 26 2020-07-31 | 374336800 | 10 | 18 | 7 | 96.190002 | 10.070000 | 1.0 | 0.0 | . 76 2020-10-12 | 240226800 | 5 | 4 | 0 | 116.970001 | 7.430000 | 1.0 | 0.0 | . 121 2020-12-15 | 157572300 | 1 | 5 | 0 | 121.779999 | 6.099998 | 1.0 | 0.0 | . 41 2020-08-21 | 338054800 | 2 | 3 | 0 | 118.275002 | 6.095001 | 1.0 | 0.0 | . 48 2020-09-01 | 152470100 | 7 | 9 | 4 | 129.039993 | 5.139999 | 1.0 | 1.0 | . 93 2020-11-04 | 138235500 | 1 | 0 | 0 | 110.440002 | 4.509995 | 1.0 | 0.0 | . 53 2020-09-09 | 176940500 | 9 | 5 | 1 | 112.820000 | 4.500000 | 1.0 | 0.0 | . 47 2020-08-31 | 225702700 | 1 | 4 | 2 | 124.807503 | 4.232491 | 1.0 | 0.0 | . 89 2020-10-29 | 146129200 | 2 | 3 | 0 | 111.199997 | 4.120003 | 1.0 | 1.0 | . 94 2020-11-05 | 126387100 | 2 | 1 | 0 | 114.949997 | 4.080002 | 1.0 | 1.0 | . pd.DataFrame(backtesting.sort_values(by=[&quot;change&quot;],ascending=True).iloc[0:10,:]) . index volume positive neutral negative close-1 change y_true y_pred . 50 2020-09-03 | 257599600 | 0 | 0 | 2 | 131.399994 | -10.519997 | 0.0 | 0.0 | . 52 2020-09-08 | 231366600 | 4 | 0 | 1 | 120.959999 | -8.139999 | 0.0 | 0.0 | . 90 2020-10-30 | 190272600 | 7 | 20 | 3 | 115.320000 | -6.459999 | 0.0 | 1.0 | . 88 2020-10-28 | 143937800 | 1 | 1 | 1 | 116.599998 | -5.400002 | 0.0 | 0.0 | . 63 2020-09-23 | 150718700 | 4 | 0 | 0 | 111.809998 | -4.689995 | 0.0 | 0.0 | . 20 2020-07-23 | 197004400 | 4 | 8 | 1 | 97.272499 | -4.427498 | 0.0 | 0.0 | . 54 2020-09-10 | 182274400 | 3 | 1 | 1 | 117.320000 | -3.830002 | 0.0 | 0.0 | . 70 2020-10-02 | 144712000 | 0 | 1 | 0 | 116.790001 | -3.770004 | 0.0 | 0.0 | . 60 2020-09-18 | 287104900 | 9 | 1 | 0 | 110.339996 | -3.500000 | 0.0 | 0.0 | . 106 2020-11-23 | 127959300 | 0 | 0 | 0 | 117.339996 | -3.489998 | 0.0 | 0.0 | . test=df[df.positive/df.negative&gt;2] . test.change.mean() . 0.10700668902010531 . test2=df[df.negative/df.positive&gt;2] . test2.change.mean() . -0.1054882838808257 . aapl = df . Pfizer . stock = &quot;Pfizer&quot; . #Configuration c = twint.Config() c.Username = (&quot;CNBC&quot;) c.Search = stock c.Since = &quot;2015-1-1&quot; c.Count = True c.Filter_retweets = True c.Pandas = True . twint.run.Search(c) . df = twint.storage.panda.Tweets_df . df_cnbc = df[[&#39;id&#39;,&#39;date&#39;,&#39;tweet&#39;,&#39;hashtags&#39;,&#39;username&#39;,&#39;search&#39;]] df_cnbc.head() . id date tweet hashtags username search . 0 1344747890607087616 | 2020-12-31 20:50:55 | Doctors criticize UK health officials for chan... | [] | CNBC | Pfizer | . 1 1341784853130469377 | 2020-12-23 16:36:52 | Stocks making the biggest moves midday: Pfizer... | [] | CNBC | Pfizer | . 2 1341781524027469825 | 2020-12-23 16:23:39 | LISTEN NOW: U.S. orders 100 million more Pfize... | [] | CNBC | Pfizer | . 3 1341778065182101504 | 2020-12-23 16:09:54 | Dubai is offering the Pfizer vaccine to reside... | [] | CNBC | Pfizer | . 4 1341737781937266690 | 2020-12-23 13:29:50 | Covid live updates: U.S. orders 100 million mo... | [] | CNBC | Pfizer | . df_cnbc.tweet = df_cnbc.tweet.apply(lambda x: sneaky_cleanup(x)) . /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . def polarity(text): return TextBlob(text).sentiment.polarity . df_cnbc[&quot;polarity&quot;] = df_cnbc[&quot;tweet&quot;].apply(polarity) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . df_cnbc.head() . id date tweet hashtags username search polarity . 0 1344747890607087616 | 2020-12-31 20:50:55 | Doctors criticize UK health official changing ... | [] | CNBC | Pfizer | 0.000000 | . 1 1341784853130469377 | 2020-12-23 16:36:52 | Stocks making biggest move FuboTV | [] | CNBC | Pfizer | 0.000000 | . 2 1341781524027469825 | 2020-12-23 16:23:39 | LISTEN order 100 million Pfizer vaccine Listen... | [] | CNBC | Pfizer | 0.000000 | . 3 1341778065182101504 | 2020-12-23 16:09:54 | Dubai offering Pfizer vaccine resident free ad... | [] | CNBC | Pfizer | 0.400000 | . 4 1341737781937266690 | 2020-12-23 13:29:50 | Covid live order 100 million Pfizer vaccine si... | [] | CNBC | Pfizer | 0.136364 | . #Configuration c = twint.Config() c.Username = (&quot;business&quot;) c.Search = stock c.Since = &quot;2015-1-1&quot; c.Count = True c.Filter_retweets = True c.Pandas = True . twint.run.Search(c) . df = twint.storage.panda.Tweets_df . df_bloom = df[[&#39;id&#39;,&#39;date&#39;,&#39;tweet&#39;,&#39;hashtags&#39;,&#39;username&#39;,&#39;search&#39;]] df_bloom . id date tweet hashtags username search . 0 1345742731520389120 | 2021-01-03 14:44:04 | Pfizer and BioNTech have offered to supply Afr... | [] | business | Pfizer | . 1 1345692721370058752 | 2021-01-03 11:25:21 | Pfizer and BioNTech have offered to supply Afr... | [] | business | Pfizer | . 2 1344704616412360705 | 2020-12-31 17:58:58 | Pfizer said the second dose of its Covid-19 va... | [] | business | Pfizer | . 3 1343983387690807299 | 2020-12-29 18:13:04 | The European Union will trigger an option to b... | [] | business | Pfizer | . 4 1343870382999293953 | 2020-12-29 10:44:01 | Dubai aims to vaccinate 70% of its population ... | [] | business | Pfizer | . ... ... | ... | ... | ... | ... | ... | . 386 608004356533821440 | 2015-06-08 20:15:09 | Internal Pfizer document warns of potential li... | [] | business | Pfizer | . 387 582567548358623232 | 2015-03-30 15:38:21 | Erections and pain are powering Pfizer&#39;s $1.4 ... | [] | business | Pfizer | . 388 563697441485365249 | 2015-02-06 13:55:17 | Pfizer&#39;s shopping spree is just getting starte... | [] | business | Pfizer | . 389 563502577896419328 | 2015-02-06 01:00:58 | Viagra-maker Pfizer just spent $17 billion on ... | [] | business | Pfizer | . 390 563327473551962112 | 2015-02-05 13:25:10 | Pfizer to buy Hospira in $17 billion deal htt... | [] | business | Pfizer | . 391 rows × 6 columns . df_bloom.tweet = df_bloom.tweet.apply(lambda x: sneaky_cleanup(x)) . /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . df_bloom[&quot;polarity&quot;] = df_bloom[&quot;tweet&quot;].apply(polarity) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . #Configuration c = twint.Config() c.Username = (&quot;WSJ&quot;) c.Search = stock c.Since = &quot;2015-1-1&quot; c.Count = True c.Filter_retweets = True c.Pandas = True . twint.run.Search(c) . df = twint.storage.panda.Tweets_df . df_wsj = df[[&#39;id&#39;,&#39;date&#39;,&#39;tweet&#39;,&#39;hashtags&#39;,&#39;username&#39;,&#39;search&#39;]] . df_wsj.tweet = df_wsj.tweet.apply(lambda x: sneaky_cleanup(x)) . /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . df_wsj[&quot;polarity&quot;] = df_wsj[&quot;tweet&quot;].apply(polarity) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . frames=[df_cnbc,df_bloom,df_wsj] . merged = pd.concat(frames) . merged[&quot;date&quot;] = pd.to_datetime(merged.date) merged[&quot;date&quot;] = merged[&quot;date&quot;] + datetime.timedelta(hours=8) . positive = merged[merged[&quot;polarity&quot;]&gt;0] neutral = merged[merged[&quot;polarity&quot;]==0] negative = merged[merged[&quot;polarity&quot;]&lt;0] . positive = positive.set_index(&#39;date&#39;).resample(&#39;D&#39;)[&#39;polarity&#39;].count() neutral = neutral.set_index(&#39;date&#39;).resample(&#39;D&#39;)[&#39;polarity&#39;].count() negative = negative.set_index(&#39;date&#39;).resample(&#39;D&#39;)[&#39;polarity&#39;].count() . positive = pd.DataFrame(positive) neutral = pd.DataFrame(neutral) negative = pd.DataFrame(negative) . positive[&quot;positive&quot;] = positive.polarity neutral[&quot;neutral&quot;]=neutral.polarity negative[&quot;negative&quot;]=negative.polarity . df2 = pd.merge(positive,neutral,left_index=True,right_index=True) . sentiment = pd.merge(df2,negative,right_index=True,left_index=True) . df = yf.download(&quot;PFE&quot;, start=start_date, end=end_date, progress=False, interval=&#39;1d&#39;) . df.columns = [w.lower() for w in df.columns] . df = pd.merge(df,sentiment,left_index=True,right_index=True) . df[&#39;close-1&#39;] = df[&#39;close&#39;].shift(+1, fill_value=df[&#39;close&#39;].iloc[0]) . df[&#39;change&#39;] = df[&quot;close&quot;] - df[&#39;close&#39;].shift(+1, fill_value=df[&#39;close&#39;].iloc[0]) #df[&#39;change_pred&#39;] = (df[&quot;close&quot;] - df[&#39;close&#39;].shift(-1, fill_value=df[&#39;change&#39;].iloc[1]))*-1 . df = df.drop([&quot;open&quot;,&quot;polarity_x&quot;,&quot;polarity_y&quot;,&quot;polarity&quot;,&quot;adj close&quot;,&quot;close&quot;,&quot;high&quot;,&quot;low&quot;],axis=1) . . Since we want to predict the closing stock price for the following day, we just shift the closing price one day to get our y value. . Since we are working with sequential data, we dont use train_test_split. Insted we pick the first 80% of obersavations as training set, and the remaning as testing set. . test_size = int(len(df) * 0.1) train = df.iloc[:-test_size,:].copy() test = df.iloc[-test_size:,:].copy() . We split the dataset into x values and y values. We also specify .values, since the date is not relevant for the training and dosn&#39;t work with some of the later preprocessing . X_train = train.iloc[:,:-1].values y_train = train.iloc[:,-1].values X_test = test.iloc[:,:-1].values y_test = test.iloc[:,-1].values . We scale all our values to be between -1 and 1. This should help the accuracy of the model. . x_scaler = MinMaxScaler(feature_range=(-1, 1)) y_scaler = MinMaxScaler(feature_range=(-1, 1)) . X_train = x_scaler.fit_transform(X_train) y_train = y_scaler.fit_transform(y_train.reshape(-1,1)) X_test = x_scaler.transform(X_test) y_test = y_scaler.transform(y_test.reshape(-1,1)) . X_train = np.reshape(X_train, (X_train.shape[0], 1, 5)) X_test = np.reshape(X_test, (X_test.shape[0], 1, 5)) . Now we start making our RNN model. . n_input = how many days we look in the past to predict the next sample. We chose 20 mostly by trial and error. . We set epochs to 100. It&#39;s our experience that the more you train the model, the more it will try to predict the daily volatility. If we only trained it for eg. 10 epochs, the model would make a soft curve which didn&#39;t look like a real stock development. By trail and error we found 100 to be the best training amount. . n_input = 10 n_features= X_train.shape[2] # how many predictors/Xs/features we have to predict y b_size = 10 # Number of timeseries samples in each batch epochs = 200 . Since we are working with sequential stock data we chose an LSTM model, which is a RNN model. Activation function is set to relu, and optimizer is adam, since these are the standard for this kind of task. We chose 2 layers with 50 units each, and once again done by trial and error. . Since we want to predict the actual stock price we use mse(mean squared error) as our loss fuction. . model = Sequential() model.add(LSTM(50, activation=&#39;relu&#39;,return_sequences=True, input_shape=(n_input, n_features))) model.add(Dropout(0.2)) model.add(LSTM(50, activation=&#39;relu&#39;)) model.add(Dense(1)) model.compile(optimizer=&#39;adam&#39;, loss=&#39;MSE&#39;,metrics=[&#39;MSE&#39;]) model.summary() . Model: &#34;sequential_32&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm_64 (LSTM) (None, 10, 50) 11200 _________________________________________________________________ dropout_32 (Dropout) (None, 10, 50) 0 _________________________________________________________________ lstm_65 (LSTM) (None, 50) 20200 _________________________________________________________________ dense_32 (Dense) (None, 1) 51 ================================================================= Total params: 31,451 Trainable params: 31,451 Non-trainable params: 0 _________________________________________________________________ . model.fit(X_train,y_train,epochs=epochs,verbose=1) . This looks like overfitting, but it works the best in our case. . loss_per_epoch = model.history.history[&#39;loss&#39;] plt.plot(range(len(loss_per_epoch)),loss_per_epoch); . Then we do some data processing to get it back to the orginal format, so we can compare the real data to the predictions. . y_pred_scaled = model.predict(X_test) y_pred = y_scaler.inverse_transform(y_pred_scaled) y_test = y_scaler.inverse_transform(y_test) results = pd.DataFrame({&#39;y_true&#39;:y_test.flatten(),&#39;y_pred&#39;:y_pred.flatten()}) print(results) . WARNING:tensorflow:Model was constructed with shape (None, 10, 5) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10, 5), dtype=tf.float32, name=&#39;lstm_64_input&#39;), name=&#39;lstm_64_input&#39;, description=&#34;created by layer &#39;lstm_64_input&#39;&#34;), but it was called on an input with incompatible shape (None, 1, 5). . WARNING:tensorflow:Model was constructed with shape (None, 10, 5) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10, 5), dtype=tf.float32, name=&#39;lstm_64_input&#39;), name=&#39;lstm_64_input&#39;, description=&#34;created by layer &#39;lstm_64_input&#39;&#34;), but it was called on an input with incompatible shape (None, 1, 5). . y_true y_pred 0 -0.550285 0.019089 1 0.123341 0.021375 2 -0.265656 -0.027585 3 0.569260 0.022268 4 0.056927 0.022450 .. ... ... 120 -1.910000 -3.817506 121 -0.500000 -1.621510 122 -0.869999 -0.010628 123 0.189999 -1.283345 124 -0.349998 -2.782472 [125 rows x 2 columns] . results[&quot;y_pred&quot;] = (results[&quot;y_pred&quot;]&gt;0).astype(int).astype(&quot;float32&quot;) results[&quot;y_true&quot;] = (results[&quot;y_true&quot;]&gt;0).astype(int).astype(&quot;float32&quot;) . backtesting = pd.merge(test.reset_index(),results,right_index=True,left_index=True) . backtesting . index volume positive neutral negative close-1 change y_true y_pred . 0 2020-06-24 | 29238800 | 0 | 0 | 0 | 31.091082 | -0.550285 | 0.0 | 1.0 | . 1 2020-06-25 | 26013900 | 0 | 0 | 0 | 30.540796 | 0.123341 | 1.0 | 1.0 | . 2 2020-06-26 | 39994300 | 0 | 0 | 0 | 30.664137 | -0.265656 | 0.0 | 0.0 | . 3 2020-06-29 | 22728900 | 0 | 0 | 0 | 30.398481 | 0.569260 | 1.0 | 1.0 | . 4 2020-06-30 | 24147700 | 0 | 1 | 0 | 30.967741 | 0.056927 | 1.0 | 1.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 120 2020-12-14 | 94809700 | 8 | 6 | 0 | 41.119999 | -1.910000 | 0.0 | 0.0 | . 121 2020-12-15 | 66003300 | 5 | 16 | 4 | 39.209999 | -0.500000 | 0.0 | 0.0 | . 122 2020-12-16 | 56515300 | 1 | 3 | 0 | 38.709999 | -0.869999 | 0.0 | 0.0 | . 123 2020-12-17 | 52036400 | 6 | 6 | 1 | 37.840000 | 0.189999 | 1.0 | 0.0 | . 124 2020-12-18 | 60259200 | 4 | 5 | 2 | 38.029999 | -0.349998 | 0.0 | 0.0 | . 125 rows × 9 columns . buy = backtesting[backtesting[&quot;y_pred&quot;]==1] sell = backtesting[backtesting[&quot;y_pred&quot;]==0] . gain = buy.change.sum() loss = sell.change.sum() . profit = gain - loss . profit . 7.696783065795898 . pd.DataFrame(backtesting.sort_values(by=[&quot;change&quot;],ascending=False).iloc[0:10,:]) . index volume positive neutral negative close-1 change y_true y_pred . 96 2020-11-09 | 230153900 | 10 | 6 | 0 | 34.535103 | 2.656548 | 1.0 | 1.0 | . 19 2020-07-22 | 90705600 | 7 | 1 | 0 | 34.810246 | 1.774193 | 1.0 | 1.0 | . 23 2020-07-28 | 56394000 | 1 | 7 | 1 | 35.616699 | 1.404175 | 1.0 | 0.0 | . 112 2020-12-02 | 84347500 | 9 | 3 | 0 | 39.410000 | 1.389999 | 1.0 | 0.0 | . 80 2020-10-16 | 42993500 | 2 | 2 | 1 | 34.677418 | 1.328274 | 1.0 | 0.0 | . 116 2020-12-08 | 87039300 | 7 | 10 | 2 | 41.250000 | 1.310001 | 1.0 | 0.0 | . 12 2020-07-13 | 56569100 | 1 | 5 | 0 | 32.096775 | 1.309296 | 1.0 | 0.0 | . 111 2020-12-01 | 72660800 | 1 | 4 | 1 | 38.310001 | 1.099998 | 1.0 | 0.0 | . 93 2020-11-04 | 40870500 | 0 | 0 | 0 | 34.335865 | 1.081593 | 1.0 | 0.0 | . 110 2020-11-30 | 65425900 | 0 | 1 | 0 | 37.230000 | 1.080002 | 1.0 | 0.0 | . pd.DataFrame(backtesting.sort_values(by=[&quot;change&quot;],ascending=True).iloc[0:10,:]) . index volume positive neutral negative close-1 change y_true y_pred . 120 2020-12-14 | 94809700 | 8 | 6 | 0 | 41.119999 | -1.910000 | 0.0 | 0.0 | . 88 2020-10-28 | 33857500 | 0 | 0 | 0 | 35.512333 | -1.878555 | 0.0 | 0.0 | . 101 2020-11-16 | 71660500 | 6 | 2 | 0 | 36.641365 | -1.223907 | 0.0 | 1.0 | . 99 2020-11-12 | 46767900 | 4 | 4 | 1 | 36.527515 | -0.901329 | 0.0 | 0.0 | . 94 2020-11-05 | 33927000 | 0 | 0 | 0 | 35.417458 | -0.891842 | 0.0 | 0.0 | . 122 2020-12-16 | 56515300 | 1 | 3 | 0 | 38.709999 | -0.869999 | 0.0 | 0.0 | . 48 2020-09-01 | 36161500 | 0 | 0 | 0 | 35.853889 | -0.863377 | 0.0 | 0.0 | . 50 2020-09-03 | 35956800 | 0 | 1 | 0 | 35.294117 | -0.759014 | 0.0 | 0.0 | . 21 2020-07-24 | 33870500 | 0 | 0 | 0 | 36.442123 | -0.711575 | 0.0 | 0.0 | . 117 2020-12-09 | 86118500 | 14 | 3 | 0 | 42.560001 | -0.710003 | 0.0 | 0.0 | . pd.crosstab(results.y_true,results.y_pred) . test=df[df.positive/df.negative&gt;5] . test.change.mean() . 0.0581401873238479 . test2=df[df.negative/df.positive&gt;5] . test2.change.mean() . 0.018975377082824707 . df.sort_values(by=&quot;positive&quot;,ascending=False) . volume positive neutral negative close-1 change . 2020-11-10 80091700 | 20 | 15 | 1 | 37.191650 | -0.493359 | . 2020-12-09 86118500 | 14 | 3 | 0 | 42.560001 | -0.710003 | . 2020-12-11 60737000 | 11 | 8 | 1 | 41.730000 | -0.610001 | . 2020-11-09 230153900 | 10 | 6 | 0 | 34.535103 | 2.656548 | . 2020-12-10 57749200 | 10 | 8 | 1 | 41.849998 | -0.119999 | . ... ... | ... | ... | ... | ... | ... | . 2017-09-06 13925800 | 0 | 0 | 0 | 32.068310 | 0.180267 | . 2017-09-05 15068600 | 0 | 0 | 0 | 32.220116 | -0.151806 | . 2017-09-01 19160800 | 0 | 1 | 0 | 32.182163 | 0.037952 | . 2017-08-31 27300500 | 0 | 0 | 0 | 31.736242 | 0.445921 | . 2018-06-26 25435000 | 0 | 0 | 0 | 34.516129 | 0.000000 | . 1260 rows × 6 columns . df.sort_values(by=&quot;negative&quot;,ascending=False) . volume positive neutral negative close-1 change . 2020-12-15 66003300 | 5 | 16 | 4 | 39.209999 | -0.500000 | . 2020-12-04 35368100 | 6 | 4 | 4 | 40.090000 | 0.250000 | . 2020-11-11 58981000 | 4 | 4 | 3 | 36.698292 | -0.170776 | . 2020-08-25 26221200 | 1 | 0 | 3 | 36.850094 | -0.407970 | . 2018-10-30 35424200 | 0 | 0 | 2 | 41.015179 | -0.322578 | . ... ... | ... | ... | ... | ... | ... | . 2017-08-25 15171200 | 0 | 0 | 0 | 31.527514 | 0.151804 | . 2017-08-24 12140400 | 0 | 0 | 0 | 31.537003 | -0.009489 | . 2017-08-23 16990000 | 0 | 0 | 0 | 31.451612 | 0.085390 | . 2017-08-22 14178600 | 0 | 0 | 0 | 31.081594 | 0.370018 | . 2018-06-26 25435000 | 0 | 0 | 0 | 34.516129 | 0.000000 | . 1260 rows × 6 columns . plt.bar(list(df.iloc[:,1:4].columns.values),df.iloc[:,1:4].sum()) . &lt;BarContainer object of 3 artists&gt; . list(df.iloc[:,1:4].columns.values) . [&#39;positive&#39;, &#39;neutral&#39;, &#39;negative&#39;] . plt.plot(df.iloc[:,1:4].sum()) . [&lt;matplotlib.lines.Line2D at 0x7f12a6da3c88&gt;] . df.iloc[:,1:4].sum() . positive 288 neutral 405 negative 65 dtype: int64 . pfe = df . Disney . stock = &quot;Disney&quot; . #Configuration c = twint.Config() c.Username = (&quot;CNBC&quot;) c.Search = stock c.Since = &quot;2015-1-1&quot; c.Count = True c.Filter_retweets = True c.Pandas = True . twint.run.Search(c) . df = twint.storage.panda.Tweets_df . df_cnbc = df[[&#39;id&#39;,&#39;date&#39;,&#39;tweet&#39;,&#39;hashtags&#39;,&#39;username&#39;,&#39;search&#39;]] df_cnbc.head() . id date tweet hashtags username search . 0 1341097244737613824 | 2020-12-21 19:04:34 | Disney shuffles execs: Taps Bergman as sole co... | [] | CNBC | Disney | . 1 1341089452215726082 | 2020-12-21 18:33:36 | Disney shuffles execs: Taps Bergman as sole co... | [] | CNBC | Disney | . 2 1341029704971804678 | 2020-12-21 14:36:11 | &#39;The Book of Boba Fett&#39; is its own series, wil... | [] | CNBC | Disney | . 3 1340342407079051266 | 2020-12-19 17:05:06 | &#39;The Mandalorian&#39; launched Disney+, now it has... | [] | CNBC | Disney | . 4 1338473326558187520 | 2020-12-14 13:18:03 | Here are Monday&#39;s biggest analyst calls of the... | [] | CNBC | Disney | . df_cnbc.tweet = df_cnbc.tweet.apply(lambda x: sneaky_cleanup(x)) . /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . def polarity(text): return TextBlob(text).sentiment.polarity . df_cnbc[&quot;polarity&quot;] = df_cnbc[&quot;tweet&quot;].apply(polarity) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . df_cnbc.head() . id date tweet hashtags username search polarity . 0 1341097244737613824 | 2020-12-21 19:04:34 | Disney shuffle Taps Bergman sole content Horn ... | [] | CNBC | Disney | 0.250000 | . 1 1341089452215726082 | 2020-12-21 18:33:36 | Disney shuffle Taps Bergman sole content Horn ... | [] | CNBC | Disney | 0.250000 | . 2 1341029704971804678 | 2020-12-21 14:36:11 | Book Boba arrive December 2021 | [] | CNBC | Disney | 0.000000 | . 3 1340342407079051266 | 2020-12-19 17:05:06 | launched goodwill usher new era Star Wars | [] | CNBC | Disney | 0.136364 | . 4 1338473326558187520 | 2020-12-14 13:18:03 | Here biggest analyst call Clorox | [] | CNBC | Disney | 0.000000 | . #Configuration c = twint.Config() c.Username = (&quot;business&quot;) c.Search = stock c.Since = &quot;2015-1-1&quot; c.Count = True c.Filter_retweets = True c.Pandas = True . twint.run.Search(c) . df = twint.storage.panda.Tweets_df . df_bloom = df[[&#39;id&#39;,&#39;date&#39;,&#39;tweet&#39;,&#39;hashtags&#39;,&#39;username&#39;,&#39;search&#39;]] df_bloom . id date tweet hashtags username search . 0 1345467910714781700 | 2021-01-02 20:32:02 | Disney spent millions to save a rainforest in ... | [] | business | Disney | . 1 1344860960037724160 | 2021-01-01 04:20:13 | ‘Queen’s Gambit’ Is Netflix’s Biggest Hit. Wou... | [] | business | Disney | . 2 1344391595165184007 | 2020-12-30 21:15:08 | Santos Laguna, a soccer club in northern Mexic... | [] | business | Disney | . 3 1343963997910007811 | 2020-12-29 16:56:01 | Netflix? HBO Max? Hulu? Disney+? There are lot... | [] | business | Disney | . 4 1343739550183682049 | 2020-12-29 02:04:08 | HBO Max and Disney+ saw jumps in app downloads... | [] | business | Disney | . ... ... | ... | ... | ... | ... | ... | . 1134 574585618610700288 | 2015-03-08 15:01:01 | Hollywood makes lots of films for kids, but th... | [] | business | Disney | . 1135 563880048181399553 | 2015-02-07 02:00:54 | Nobody wants Disney&#39;s CEO to leave http://t.c... | [] | business | Disney | . 1136 563132578493112320 | 2015-02-05 00:30:43 | For an instant, it looked like there was a mas... | [] | business | Disney | . 1137 560958225135448065 | 2015-01-30 00:30:37 | Disney unveils its first Latina Princess http... | [] | business | Disney | . 1138 555869549174599681 | 2015-01-15 23:30:02 | Most measles patients in Disney cases unvaccin... | [] | business | Disney | . 1139 rows × 6 columns . df_bloom.tweet = df_bloom.tweet.apply(lambda x: sneaky_cleanup(x)) . /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . df_bloom[&quot;polarity&quot;] = df_bloom[&quot;tweet&quot;].apply(polarity) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . #Configuration c = twint.Config() c.Username = (&quot;WSJ&quot;) c.Search = stock c.Since = &quot;2015-1-1&quot; c.Count = True c.Filter_retweets = True c.Pandas = True . twint.run.Search(c) . df = twint.storage.panda.Tweets_df . df_wsj = df[[&#39;id&#39;,&#39;date&#39;,&#39;tweet&#39;,&#39;hashtags&#39;,&#39;username&#39;,&#39;search&#39;]] . df_wsj.tweet = df_wsj.tweet.apply(lambda x: sneaky_cleanup(x)) . /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . df_wsj[&quot;polarity&quot;] = df_wsj[&quot;tweet&quot;].apply(polarity) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . frames=[df_cnbc,df_bloom,df_wsj] . merged = pd.concat(frames) . merged[&quot;date&quot;] = pd.to_datetime(merged.date) merged[&quot;date&quot;] = merged[&quot;date&quot;] + datetime.timedelta(hours=8) . positive = merged[merged[&quot;polarity&quot;]&gt;0] neutral = merged[merged[&quot;polarity&quot;]==0] negative = merged[merged[&quot;polarity&quot;]&lt;0] . positive = positive.set_index(&#39;date&#39;).resample(&#39;D&#39;)[&#39;polarity&#39;].count() neutral = neutral.set_index(&#39;date&#39;).resample(&#39;D&#39;)[&#39;polarity&#39;].count() negative = negative.set_index(&#39;date&#39;).resample(&#39;D&#39;)[&#39;polarity&#39;].count() . positive = pd.DataFrame(positive) neutral = pd.DataFrame(neutral) negative = pd.DataFrame(negative) . positive[&quot;positive&quot;] = positive.polarity neutral[&quot;neutral&quot;]=neutral.polarity negative[&quot;negative&quot;]=negative.polarity . df2 = pd.merge(positive,neutral,left_index=True,right_index=True) . sentiment = pd.merge(df2,negative,right_index=True,left_index=True) . df = yf.download(&quot;DIS&quot;, start=start_date, end=end_date, progress=False, interval=&#39;1d&#39;) . df.columns = [w.lower() for w in df.columns] . df = pd.merge(df,sentiment,left_index=True,right_index=True) . df[&#39;close-1&#39;] = df[&#39;close&#39;].shift(+1, fill_value=df[&#39;close&#39;].iloc[0]) . df[&#39;change&#39;] = df[&quot;close&quot;] - df[&#39;close&#39;].shift(+1, fill_value=df[&#39;close&#39;].iloc[0]) #df[&quot;day_change&quot;]=(df[&quot;open&quot;] - df[&#39;close&#39;])*-1 #df[&#39;change_pred&#39;] = (df[&quot;close&quot;] - df[&#39;close&#39;].shift(-1, fill_value=df[&#39;change&#39;].iloc[1]))*-1 . df = df.drop([&quot;open&quot;,&quot;polarity_x&quot;,&quot;polarity_y&quot;,&quot;polarity&quot;,&quot;adj close&quot;,&quot;close&quot;,&quot;high&quot;,&quot;low&quot;],axis=1) #df= df.drop([&quot;adj close&quot;,&quot;close&quot;,&quot;high&quot;,&quot;low&quot;],axis=1) . df . volume positive neutral negative close-1 change . 2015-12-23 12372900 | 0 | 2 | 2 | 105.559998 | 0.000000 | . 2015-12-24 4356100 | 0 | 0 | 0 | 105.559998 | 0.300003 | . 2015-12-28 9092700 | 0 | 0 | 0 | 105.860001 | 1.389999 | . 2015-12-29 8607200 | 0 | 0 | 0 | 107.250000 | -0.169998 | . 2015-12-30 4917000 | 0 | 0 | 0 | 107.080002 | -0.740005 | . ... ... | ... | ... | ... | ... | ... | . 2020-12-14 30939400 | 3 | 2 | 1 | 175.720001 | -6.419998 | . 2020-12-15 18817200 | 1 | 0 | 0 | 169.300003 | 4.639999 | . 2020-12-16 11105800 | 0 | 1 | 0 | 173.940002 | -0.820007 | . 2020-12-17 9004600 | 0 | 0 | 0 | 173.119995 | 0.430008 | . 2020-12-18 21172100 | 1 | 1 | 1 | 173.550003 | -0.660004 | . 1257 rows × 6 columns . . Since we want to predict the closing stock price for the following day, we just shift the closing price one day to get our y value. . Since we are working with sequential data, we dont use train_test_split. Insted we pick the first 80% of obersavations as training set, and the remaning as testing set. . test_size = int(len(df) * 0.1) train = df.iloc[:-test_size,:].copy() test = df.iloc[-test_size:,:].copy() . We split the dataset into x values and y values. We also specify .values, since the date is not relevant for the training and dosn&#39;t work with some of the later preprocessing . X_train = train.iloc[:,:-1].values y_train = train.iloc[:,-1].values X_test = test.iloc[:,:-1].values y_test = test.iloc[:,-1].values . from sklearn.preprocessing import MinMaxScaler . We scale all our values to be between -1 and 1. This should help the accuracy of the model. . x_scaler = MinMaxScaler(feature_range=(-1, 1)) y_scaler = MinMaxScaler(feature_range=(-1, 1)) . X_train = x_scaler.fit_transform(X_train) y_train = y_scaler.fit_transform(y_train.reshape(-1,1)) X_test = x_scaler.transform(X_test) y_test = y_scaler.transform(y_test.reshape(-1,1)) . X_train = np.reshape(X_train, (X_train.shape[0], 1, 5)) X_test = np.reshape(X_test, (X_test.shape[0], 1, 5)) . from keras.preprocessing.sequence import TimeseriesGenerator . Now we start making our RNN model. . n_input = how many days we look in the past to predict the next sample. We chose 20 mostly by trial and error. . We set epochs to 100. It&#39;s our experience that the more you train the model, the more it will try to predict the daily volatility. If we only trained it for eg. 10 epochs, the model would make a soft curve which didn&#39;t look like a real stock development. By trail and error we found 100 to be the best training amount. . n_input = 10 n_features= X_train.shape[2] # how many predictors/Xs/features we have to predict y b_size = 10 # Number of timeseries samples in each batch epochs = 200 . import numpy as np . import keras from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM from keras.layers import Dropout . Since we are working with sequential stock data we chose an LSTM model, which is a RNN model. Activation function is set to relu, and optimizer is adam, since these are the standard for this kind of task. We chose 2 layers with 50 units each, and once again done by trial and error. . Since we want to predict the actual stock price we use mse(mean squared error) as our loss fuction. . model = Sequential() model.add(LSTM(50, activation=&#39;relu&#39;,return_sequences=True, input_shape=(n_input, n_features))) model.add(Dropout(0.2)) model.add(LSTM(50, activation=&#39;relu&#39;)) model.add(Dense(1)) model.compile(optimizer=&#39;adam&#39;, loss=&#39;MSE&#39;,metrics=[&#39;MSE&#39;]) model.summary() . Model: &#34;sequential_42&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm_84 (LSTM) (None, 10, 50) 11200 _________________________________________________________________ dropout_42 (Dropout) (None, 10, 50) 0 _________________________________________________________________ lstm_85 (LSTM) (None, 50) 20200 _________________________________________________________________ dense_42 (Dense) (None, 1) 51 ================================================================= Total params: 31,451 Trainable params: 31,451 Non-trainable params: 0 _________________________________________________________________ . model.fit(X_train,y_train,epochs=epochs,verbose=1) . import matplotlib.pyplot as plt . This looks like overfitting, but it works the best in our case. . loss_per_epoch = model.history.history[&#39;loss&#39;] plt.plot(range(len(loss_per_epoch)),loss_per_epoch); . Then we do some data processing to get it back to the orginal format, so we can compare the real data to the predictions. . y_pred_scaled = model.predict(X_test) y_pred = y_scaler.inverse_transform(y_pred_scaled) y_test = y_scaler.inverse_transform(y_test) results = pd.DataFrame({&#39;y_true&#39;:y_test.flatten(),&#39;y_pred&#39;:y_pred.flatten()}) print(results) . WARNING:tensorflow:Model was constructed with shape (None, 10, 5) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10, 5), dtype=tf.float32, name=&#39;lstm_84_input&#39;), name=&#39;lstm_84_input&#39;, description=&#34;created by layer &#39;lstm_84_input&#39;&#34;), but it was called on an input with incompatible shape (None, 1, 5). . WARNING:tensorflow:Model was constructed with shape (None, 10, 5) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10, 5), dtype=tf.float32, name=&#39;lstm_84_input&#39;), name=&#39;lstm_84_input&#39;, description=&#34;created by layer &#39;lstm_84_input&#39;&#34;), but it was called on an input with incompatible shape (None, 1, 5). . y_true y_pred 0 -4.519997 -0.954894 1 -0.709999 0.084928 2 -2.260002 0.060028 3 2.419998 0.079551 4 -0.009995 0.113378 .. ... ... 120 -6.419998 -11.621860 121 4.639999 -6.909235 122 -0.820007 -8.227471 123 0.430008 -7.551184 124 -0.660004 -9.382707 [125 rows x 2 columns] . results[&quot;y_pred&quot;] = (results[&quot;y_pred&quot;]&gt;0).astype(int).astype(&quot;float32&quot;) results[&quot;y_true&quot;] = (results[&quot;y_true&quot;]&gt;0).astype(int).astype(&quot;float32&quot;) . backtesting = pd.merge(test.reset_index(),results,right_index=True,left_index=True) . buy = backtesting[backtesting[&quot;y_pred&quot;]==1] sell = backtesting[backtesting[&quot;y_pred&quot;]==0] . backtesting . index volume positive neutral negative close-1 change y_true y_pred . 0 2020-06-24 | 22252500 | 0 | 2 | 0 | 116.589996 | -4.519997 | 0.0 | 0.0 | . 1 2020-06-25 | 17240400 | 1 | 4 | 0 | 112.070000 | -0.709999 | 0.0 | 1.0 | . 2 2020-06-26 | 15270900 | 2 | 5 | 0 | 111.360001 | -2.260002 | 0.0 | 1.0 | . 3 2020-06-29 | 12584300 | 2 | 0 | 0 | 109.099998 | 2.419998 | 1.0 | 1.0 | . 4 2020-06-30 | 10565900 | 0 | 0 | 0 | 111.519997 | -0.009995 | 0.0 | 1.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 120 2020-12-14 | 30939400 | 3 | 2 | 1 | 175.720001 | -6.419998 | 0.0 | 0.0 | . 121 2020-12-15 | 18817200 | 1 | 0 | 0 | 169.300003 | 4.639999 | 1.0 | 0.0 | . 122 2020-12-16 | 11105800 | 0 | 1 | 0 | 173.940002 | -0.820007 | 0.0 | 0.0 | . 123 2020-12-17 | 9004600 | 0 | 0 | 0 | 173.119995 | 0.430008 | 1.0 | 0.0 | . 124 2020-12-18 | 21172100 | 1 | 1 | 1 | 173.550003 | -0.660004 | 0.0 | 0.0 | . 125 rows × 9 columns . gain = buy.change.sum() loss = sell.change.sum() . profit = gain - loss . profit . 57.37998962402344 . pd.DataFrame(backtesting.sort_values(by=[&quot;change&quot;],ascending=False).iloc[0:10,:]) . index volume positive neutral negative close-1 change y_true y_pred . 119 2020-12-11 | 87410700 | 11 | 5 | 1 | 154.690002 | 21.029999 | 1.0 | 1.0 | . 96 2020-11-09 | 35634700 | 0 | 0 | 0 | 127.459999 | 15.129997 | 1.0 | 1.0 | . 29 2020-08-05 | 53122400 | 8 | 11 | 2 | 117.290001 | 10.320000 | 1.0 | 1.0 | . 101 2020-11-16 | 16382500 | 0 | 0 | 0 | 138.360001 | 6.309998 | 1.0 | 1.0 | . 107 2020-11-24 | 16711700 | 0 | 0 | 0 | 145.979996 | 5.510010 | 1.0 | 1.0 | . 106 2020-11-23 | 12887000 | 0 | 0 | 0 | 141.070007 | 4.909988 | 1.0 | 0.0 | . 121 2020-12-15 | 18817200 | 1 | 0 | 0 | 169.300003 | 4.639999 | 1.0 | 0.0 | . 112 2020-12-02 | 10601900 | 3 | 1 | 0 | 149.440002 | 4.169998 | 1.0 | 0.0 | . 77 2020-10-13 | 22389000 | 7 | 8 | 2 | 124.970001 | 3.990005 | 1.0 | 0.0 | . 92 2020-11-03 | 8144900 | 0 | 0 | 0 | 120.129997 | 3.889999 | 1.0 | 1.0 | . pd.DataFrame(backtesting.sort_values(by=[&quot;change&quot;],ascending=True).iloc[0:10,:]) . index volume positive neutral negative close-1 change y_true y_pred . 120 2020-12-14 | 30939400 | 3 | 2 | 1 | 175.720001 | -6.419998 | 0.0 | 0.0 | . 88 2020-10-28 | 11654700 | 0 | 0 | 0 | 123.309998 | -4.839996 | 0.0 | 1.0 | . 0 2020-06-24 | 22252500 | 0 | 2 | 0 | 116.589996 | -4.519997 | 0.0 | 0.0 | . 86 2020-10-26 | 8578900 | 0 | 0 | 0 | 128.350006 | -4.290009 | 0.0 | 1.0 | . 98 2020-11-11 | 9600900 | 0 | 1 | 0 | 142.110001 | -4.289993 | 0.0 | 0.0 | . 63 2020-09-23 | 8323600 | 0 | 0 | 0 | 127.209999 | -3.930000 | 0.0 | 1.0 | . 47 2020-08-31 | 11617500 | 0 | 3 | 0 | 135.539993 | -3.669998 | 0.0 | 0.0 | . 61 2020-09-21 | 10352700 | 0 | 0 | 0 | 128.630005 | -3.220001 | 0.0 | 1.0 | . 12 2020-07-13 | 15620200 | 2 | 3 | 0 | 119.339996 | -3.119995 | 0.0 | 1.0 | . 81 2020-10-19 | 6576900 | 0 | 0 | 0 | 126.809998 | -2.579994 | 0.0 | 1.0 | . pd.crosstab(results.y_true,results.y_pred) . y_pred 0.0 1.0 . y_true . 0.0 52 | 14 | . 1.0 43 | 16 | . test=df[df.positive/df.negative&gt;5] . test.change.mean() . 0.15256644279907586 . test2=df[df.negative/df.positive&gt;2] . test2.change.mean() . -0.3712306096003606 . dis = df . plt.bar(list(df.iloc[:,1:4].columns.values),df.iloc[:,1:4].sum()) . &lt;BarContainer object of 3 artists&gt; . import numpy as np import matplotlib.pyplot as plt fig = plt.figure() ax = fig.add_axes([0,0,1,1]) ax.bar((list(df.iloc[:,1:4].columns.values))+0,amzn.iloc[:,1:4].sum(), color = &#39;b&#39;, width = 0.25) ax.bar((list(df.iloc[:,1:4].columns.values))+0.25,aapl.iloc[:,1:4].sum(), color = &#39;r&#39;, width = 0.25) ax.bar((list(df.iloc[:,1:4].columns.values))+0.5,pfe.iloc[:,1:4].sum(), color = &#39;g&#39;, width = 0.25) . TypeError Traceback (most recent call last) &lt;ipython-input-355-bf8dc270717d&gt; in &lt;module&gt;() 3 fig = plt.figure() 4 ax = fig.add_axes([0,0,1,1]) -&gt; 5 ax.bar((list(df.iloc[:,1:4].columns.values)+0),amzn.iloc[:,1:4].sum(), color = &#39;b&#39;, width = 0.25) 6 ax.bar((list(df.iloc[:,1:4].columns.values))+0.25,aapl.iloc[:,1:4].sum(), color = &#39;r&#39;, width = 0.25) 7 ax.bar((list(df.iloc[:,1:4].columns.values))+0.5,pfe.iloc[:,1:4].sum(), color = &#39;g&#39;, width = 0.25) TypeError: can only concatenate list (not &#34;int&#34;) to list . import numpy as np import matplotlib.pyplot as plt fig = plt.figure() ax = fig.add_axes([2,2,2,2]) X = np.arange(3) ax.bar(X +0,aapl.iloc[:,1:4].sum(), color = &#39;r&#39;, width = 0.20,label=&quot;Apple&quot;) ax.bar(X + 0.20,amzn.iloc[:,1:4].sum(), color = &#39;b&#39;, width = 0.20,label=&quot;Amazon&quot;) ax.bar(X +0.40,dis.iloc[:,1:4].sum(), color = &#39;y&#39;, width = 0.20,label=&quot;Disney&quot;) ax.bar(X +0.60,pfe.iloc[:,1:4].sum(), color = &#39;g&#39;, width = 0.20,label=&quot;Pfizer&quot;) plt.xlabel(&#39;Sentiment&#39;, fontweight=&#39;bold&#39;,fontsize=18) plt.ylabel(&#39;News&#39;, fontweight=&#39;bold&#39;,fontsize=18) plt.xticks([r + 0.3 for r in range(len(amzn.iloc[:,1:4].sum()))], ticker,fontsize=13) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f129c97d9b0&gt; . import numpy as np import matplotlib.pyplot as plt fig = plt.figure() ax = fig.add_axes([2,2,2,2]) X = np.arange(3) ax.bar(X +0,aapl.iloc[:,1:4].sum()/sum(aapl.iloc[:,1:4].sum())*100, color = &#39;r&#39;, width = 0.20,label=&quot;Apple&quot;) ax.bar(X + 0.20,amzn.iloc[:,1:4].sum()/sum(amzn.iloc[:,1:4].sum())*100, color = &#39;b&#39;, width = 0.20,label=&quot;Amazon&quot;) ax.bar(X +0.40,dis.iloc[:,1:4].sum()/sum(dis.iloc[:,1:4].sum())*100, color = &#39;y&#39;, width = 0.20,label=&quot;Disney&quot;) ax.bar(X +0.60,pfe.iloc[:,1:4].sum()/sum(pfe.iloc[:,1:4].sum())*100, color = &#39;g&#39;, width = 0.20,label=&quot;Pfizer&quot;) plt.xlabel(&#39;Sentiment in %&#39;, fontweight=&#39;bold&#39;,fontsize=18) plt.ylabel(&#39;News&#39;, fontweight=&#39;bold&#39;,fontsize=18) plt.xticks([r + 0.3 for r in range(len(amzn.iloc[:,1:4].sum()))], ticker,fontsize=13) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f129c906470&gt; . sum(aapl.iloc[:,1:4].sum())/aapl.iloc[:,1:4].sum() . positive 2.682576 neutral 1.936120 negative 9.031201 dtype: float64 . aapl.iloc[:,1:4].sum()/sum(aapl.iloc[:,1:4].sum())*100 . positive 37.277595 neutral 51.649680 negative 11.072724 dtype: float64 .",
            "url": "https://msvante.github.io/Notebook-blogs/2021/01/08/stock_predictions.html",
            "relUrl": "/2021/01/08/stock_predictions.html",
            "date": " • Jan 8, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://msvante.github.io/Notebook-blogs/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://msvante.github.io/Notebook-blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://msvante.github.io/Notebook-blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}